{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create initial DF\n",
    "\n",
    "df = pd.read_csv('ChurnData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3333 entries, 0 to 3332\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   state                   3333 non-null   object \n",
      " 1   account length          3333 non-null   int64  \n",
      " 2   area code               3333 non-null   int64  \n",
      " 3   phone number            3333 non-null   object \n",
      " 4   international plan      3333 non-null   object \n",
      " 5   voice mail plan         3333 non-null   object \n",
      " 6   number vmail messages   3333 non-null   int64  \n",
      " 7   total day minutes       3333 non-null   float64\n",
      " 8   total day calls         3333 non-null   int64  \n",
      " 9   total day charge        3333 non-null   float64\n",
      " 10  total eve minutes       3333 non-null   float64\n",
      " 11  total eve calls         3333 non-null   int64  \n",
      " 12  total eve charge        3333 non-null   float64\n",
      " 13  total night minutes     3333 non-null   float64\n",
      " 14  total night calls       3333 non-null   int64  \n",
      " 15  total night charge      3333 non-null   float64\n",
      " 16  total intl minutes      3333 non-null   float64\n",
      " 17  total intl calls        3333 non-null   int64  \n",
      " 18  total intl charge       3333 non-null   float64\n",
      " 19  customer service calls  3333 non-null   int64  \n",
      " 20  churn                   3333 non-null   bool   \n",
      "dtypes: bool(1), float64(8), int64(8), object(4)\n",
      "memory usage: 524.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Checking Info\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3333, 21)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data set dimnensions\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account length</th>\n",
       "      <th>area code</th>\n",
       "      <th>number vmail messages</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total day calls</th>\n",
       "      <th>total day charge</th>\n",
       "      <th>total eve minutes</th>\n",
       "      <th>total eve calls</th>\n",
       "      <th>total eve charge</th>\n",
       "      <th>total night minutes</th>\n",
       "      <th>total night calls</th>\n",
       "      <th>total night charge</th>\n",
       "      <th>total intl minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>total intl charge</th>\n",
       "      <th>customer service calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101.064806</td>\n",
       "      <td>437.182418</td>\n",
       "      <td>8.099010</td>\n",
       "      <td>179.775098</td>\n",
       "      <td>100.435644</td>\n",
       "      <td>30.562307</td>\n",
       "      <td>200.980348</td>\n",
       "      <td>100.114311</td>\n",
       "      <td>17.083540</td>\n",
       "      <td>200.872037</td>\n",
       "      <td>100.107711</td>\n",
       "      <td>9.039325</td>\n",
       "      <td>10.237294</td>\n",
       "      <td>4.479448</td>\n",
       "      <td>2.764581</td>\n",
       "      <td>1.562856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.822106</td>\n",
       "      <td>42.371290</td>\n",
       "      <td>13.688365</td>\n",
       "      <td>54.467389</td>\n",
       "      <td>20.069084</td>\n",
       "      <td>9.259435</td>\n",
       "      <td>50.713844</td>\n",
       "      <td>19.922625</td>\n",
       "      <td>4.310668</td>\n",
       "      <td>50.573847</td>\n",
       "      <td>19.568609</td>\n",
       "      <td>2.275873</td>\n",
       "      <td>2.791840</td>\n",
       "      <td>2.461214</td>\n",
       "      <td>0.753773</td>\n",
       "      <td>1.315491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.700000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>24.430000</td>\n",
       "      <td>166.600000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>14.160000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.400000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>201.400000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>201.200000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.050000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>216.400000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>36.790000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>243.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>350.800000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>363.700000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>30.910000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>17.770000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       account length    area code  number vmail messages  total day minutes  \\\n",
       "count     3333.000000  3333.000000            3333.000000        3333.000000   \n",
       "mean       101.064806   437.182418               8.099010         179.775098   \n",
       "std         39.822106    42.371290              13.688365          54.467389   \n",
       "min          1.000000   408.000000               0.000000           0.000000   \n",
       "25%         74.000000   408.000000               0.000000         143.700000   \n",
       "50%        101.000000   415.000000               0.000000         179.400000   \n",
       "75%        127.000000   510.000000              20.000000         216.400000   \n",
       "max        243.000000   510.000000              51.000000         350.800000   \n",
       "\n",
       "       total day calls  total day charge  total eve minutes  total eve calls  \\\n",
       "count      3333.000000       3333.000000        3333.000000      3333.000000   \n",
       "mean        100.435644         30.562307         200.980348       100.114311   \n",
       "std          20.069084          9.259435          50.713844        19.922625   \n",
       "min           0.000000          0.000000           0.000000         0.000000   \n",
       "25%          87.000000         24.430000         166.600000        87.000000   \n",
       "50%         101.000000         30.500000         201.400000       100.000000   \n",
       "75%         114.000000         36.790000         235.300000       114.000000   \n",
       "max         165.000000         59.640000         363.700000       170.000000   \n",
       "\n",
       "       total eve charge  total night minutes  total night calls  \\\n",
       "count       3333.000000          3333.000000        3333.000000   \n",
       "mean          17.083540           200.872037         100.107711   \n",
       "std            4.310668            50.573847          19.568609   \n",
       "min            0.000000            23.200000          33.000000   \n",
       "25%           14.160000           167.000000          87.000000   \n",
       "50%           17.120000           201.200000         100.000000   \n",
       "75%           20.000000           235.300000         113.000000   \n",
       "max           30.910000           395.000000         175.000000   \n",
       "\n",
       "       total night charge  total intl minutes  total intl calls  \\\n",
       "count         3333.000000         3333.000000       3333.000000   \n",
       "mean             9.039325           10.237294          4.479448   \n",
       "std              2.275873            2.791840          2.461214   \n",
       "min              1.040000            0.000000          0.000000   \n",
       "25%              7.520000            8.500000          3.000000   \n",
       "50%              9.050000           10.300000          4.000000   \n",
       "75%             10.590000           12.100000          6.000000   \n",
       "max             17.770000           20.000000         20.000000   \n",
       "\n",
       "       total intl charge  customer service calls  \n",
       "count        3333.000000             3333.000000  \n",
       "mean            2.764581                1.562856  \n",
       "std             0.753773                1.315491  \n",
       "min             0.000000                0.000000  \n",
       "25%             2.300000                1.000000  \n",
       "50%             2.780000                1.000000  \n",
       "75%             3.270000                2.000000  \n",
       "max             5.400000                9.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data set\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2850\n",
       "True      483\n",
       "Name: churn, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Famliarizing with Target Variable\n",
    "\n",
    "df['churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.855086\n",
       "True     0.144914\n",
       "Name: churn, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Further investigation of target variable\n",
    "\n",
    "df['churn'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Define X & Y\n",
    "\n",
    "X = df.drop('churn', axis = 1)\n",
    "y = df['churn']\n",
    "\n",
    "#Train Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns: \n",
      "['account length', 'area code', 'number vmail messages', 'total day minutes', 'total day calls', 'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes', 'total night calls', 'total night charge', 'total intl minutes', 'total intl calls', 'total intl charge', 'customer service calls']\n",
      "==\n",
      "OHE Columns: \n",
      "['international plan', 'voice mail plan']\n",
      "==\n",
      "Categorical Columns that will not OHE: \n",
      "['state', 'phone number']\n"
     ]
    }
   ],
   "source": [
    "#Get numeric cols, categorical cols, and cols to frequency\n",
    "\n",
    "#taken from: https://github.com/flatiron-school/Online-DS-PT-022221/blob/main/Phase3/complete_notebooks/32-Pipelines_complete.ipynb\n",
    "\n",
    "num_cols = []\n",
    "cols_to_ohe = []\n",
    "cols_to_freq = []\n",
    "\n",
    "for c in X_train.columns:\n",
    "    # Want to grab numeric columns\n",
    "    if X_train[c].dtype in ['float64', 'int64']:\n",
    "        # same as if X_train[c].dtype == 'float64'\n",
    "        num_cols.append(c)\n",
    "\n",
    "    # Then grab columns with fewer than 10 unique values\n",
    "    elif len(X_train[c].unique()) < 10:\n",
    "        cols_to_ohe.append(c)\n",
    "\n",
    "    # Then grab columns with more than 10, since we won't OHE those\n",
    "    else:\n",
    "        cols_to_freq.append(c)\n",
    "        \n",
    "print(f'Numeric Columns: ')\n",
    "print(num_cols)\n",
    "print(f'==')\n",
    "\n",
    "print(f'OHE Columns: ')\n",
    "print(cols_to_ohe)\n",
    "print(f'==')\n",
    "\n",
    "print(f'Categorical Columns that will not OHE: ')\n",
    "print(cols_to_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Pre Processing Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "#Sourced From: https://github.com/flatiron-school/Online-DS-PT-022221/blob/main/Phase3/complete_notebooks/32-Pipelines_complete.ipynb\n",
    "\n",
    "# Numerical Pre Processing Steps:\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "#OHE Pre processing Steps:\n",
    "\n",
    "ohe_transformer = Pipeline(steps=[\n",
    "    ('ohe_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohencoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Frequency Columns Pre processing Steps:\n",
    "\n",
    "freq_transformer = Pipeline(steps=[\n",
    "    ('freq_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('freq_enc', ce.count.CountEncoder(normalize=True, \n",
    "                                       handle_unknown=0,\n",
    "                                       min_group_size=0.001,\n",
    "                                       min_group_name='Other'))])\n",
    "\n",
    "#Put all Processing Steps together in Columntransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('ohe', ohe_transformer, cols_to_ohe),\n",
    "        ('freq', freq_transformer, cols_to_freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit preprocessor to X_Train data\n",
    "\n",
    "X_train_proc = pd.DataFrame(preprocessor.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature names from processor\n",
    "\n",
    "#Sourced from: https://johaupt.github.io/scikit-learn/tutorial/python/data%20processing/ml%20pipeline/model%20interpretation/columnTransformer_feature_names.html\n",
    "\n",
    "import warnings\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-0d499a1f2f22>:38: UserWarning: Transformer num_imputer (type SimpleImputer) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "<ipython-input-139-0d499a1f2f22>:38: UserWarning: Transformer scaler (type MinMaxScaler) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "<ipython-input-139-0d499a1f2f22>:38: UserWarning: Transformer ohe_imputer (type SimpleImputer) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "<ipython-input-139-0d499a1f2f22>:38: UserWarning: Transformer freq_imputer (type SimpleImputer) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "<ipython-input-139-0d499a1f2f22>:38: UserWarning: Transformer freq_enc (type CountEncoder) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n"
     ]
    }
   ],
   "source": [
    "#Get feature names of preprocessor\n",
    "\n",
    "processed_feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "#Assign column names to preprocessed X_train_cols\n",
    "\n",
    "X_train_proc.columns = [processed_feature_names]\n",
    "\n",
    "#transform X_test\n",
    "\n",
    "X_test_proc = pd.DataFrame(preprocessor.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that runs all models to determine most efficient model\n",
    "\n",
    "def multi_model_test(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Import metrics assessment tools\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "    \n",
    "    ###Running Logistic Regression Model###\n",
    "    \n",
    "    #Import LogisticRegression\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    #Train Model\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    #Create LR Predictions\n",
    "    \n",
    "    lr_pred_train = lr.predict(X_train)\n",
    "    lr_pred_test = lr.predict(X_test)\n",
    "    \n",
    "    #Get LR Evaluation Metrics:\n",
    "    \n",
    "    #Get Evaluation Metrics for LR Train Set:\n",
    "    \n",
    "    lr_train_precision = precision_score(lr_pred_train, y_train)\n",
    "    lr_train_recall = recall_score(lr_pred_train, y_train)\n",
    "    lr_train_accuracy = accuracy_score(lr_pred_train, y_train)\n",
    "    lr_train_f1 = f1_score(lr_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for LR Test Set:\n",
    "\n",
    "    lr_test_precision = precision_score(lr_pred_test, y_test)\n",
    "    lr_test_recall = recall_score(lr_pred_test, y_test)\n",
    "    lr_test_accuracy = accuracy_score(lr_pred_test, y_test)\n",
    "    lr_test_f1 = f1_score(lr_pred_test, y_test)\n",
    "    \n",
    "    ###Running K Nearest Neighbors Model###\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    KNN = KNeighborsClassifier()\n",
    "    \n",
    "    #Fit KNN model\n",
    "    \n",
    "    KNN.fit(X_train, y_train)\n",
    "    \n",
    "    KNN_pred_train = KNN.predict(X_train)\n",
    "    \n",
    "    KNN_pred_test = KNN.predict(X_test)\n",
    "\n",
    "    #Get Evaluation Metrics for KNN Train Set:\n",
    "    \n",
    "    KNN_train_precision = precision_score(KNN_pred_train, y_train)\n",
    "    KNN_train_recall = recall_score(KNN_pred_train, y_train)\n",
    "    KNN_train_accuracy = accuracy_score(KNN_pred_train, y_train)\n",
    "    KNN_train_f1 = f1_score(KNN_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for KNN Test Set:\n",
    "\n",
    "    KNN_test_precision = precision_score(KNN_pred_test, y_test)\n",
    "    KNN_test_recall = recall_score(KNN_pred_test, y_test)\n",
    "    KNN_test_accuracy = accuracy_score(KNN_pred_test, y_test)\n",
    "    KNN_test_f1 = f1_score(KNN_pred_test, y_test)\n",
    "    \n",
    "    ###Running Decision Tree Model###\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    DTC = DecisionTreeClassifier()\n",
    "    \n",
    "    #Fit DTC model\n",
    "    \n",
    "    DTC.fit(X_train, y_train)\n",
    "    \n",
    "    DTC_pred_train = DTC.predict(X_train)\n",
    "    \n",
    "    DTC_pred_test = DTC.predict(X_test)\n",
    "\n",
    "    #Get Evaluation Metrics for DTC Train Set:\n",
    "    \n",
    "    DTC_train_precision = precision_score(DTC_pred_train, y_train)\n",
    "    DTC_train_recall = recall_score(DTC_pred_train, y_train)\n",
    "    DTC_train_accuracy = accuracy_score(DTC_pred_train, y_train)\n",
    "    DTC_train_f1 = f1_score(DTC_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for DTC Test Set:\n",
    "\n",
    "    DTC_test_precision = precision_score(DTC_pred_test, y_test)\n",
    "    DTC_test_recall = recall_score(DTC_pred_test, y_test)\n",
    "    DTC_test_accuracy = accuracy_score(DTC_pred_test, y_test)\n",
    "    DTC_test_f1 = f1_score(DTC_pred_test, y_test)\n",
    "\n",
    "    ###Running Decision Tree Regression (CART) Model###\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    \n",
    "    DTR = DecisionTreeRegressor()\n",
    "    \n",
    "    #Fit DTR model\n",
    "    \n",
    "    DTR.fit(X_train, y_train)\n",
    "    \n",
    "    DTR_pred_train = DTR.predict(X_train)\n",
    "    \n",
    "    DTR_pred_test = DTR.predict(X_test)\n",
    "\n",
    "    #Get Evaluation Metrics for DTR Train Set:\n",
    "    \n",
    "    DTR_train_precision = precision_score(DTR_pred_train, y_train)\n",
    "    DTR_train_recall = recall_score(DTR_pred_train, y_train)\n",
    "    DTR_train_accuracy = accuracy_score(DTR_pred_train, y_train)\n",
    "    DTR_train_f1 = f1_score(DTR_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for DTR Test Set:\n",
    "\n",
    "    DTR_test_precision = precision_score(DTR_pred_test, y_test)\n",
    "    DTR_test_recall = recall_score(DTR_pred_test, y_test)\n",
    "    DTR_test_accuracy = accuracy_score(DTR_pred_test, y_test)\n",
    "    DTR_test_f1 = f1_score(DTR_pred_test, y_test)\n",
    "    \n",
    "    ###Running Random Forests###\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    forest = RandomForestClassifier(n_estimators = 100, max_depth = 5)\n",
    "    \n",
    "    #Fit DTR model\n",
    "    \n",
    "    forest.fit(X_train, y_train)\n",
    "    \n",
    "    forest_pred_train = forest.predict(X_train)\n",
    "    \n",
    "    forest_pred_test = forest.predict(X_test)\n",
    "\n",
    "    #Get Evaluation Metrics for DTR Train Set:\n",
    "    \n",
    "    forest_train_precision = precision_score(forest_pred_train, y_train)\n",
    "    forest_train_recall = recall_score(forest_pred_train, y_train)\n",
    "    forest_train_accuracy = accuracy_score(forest_pred_train, y_train)\n",
    "    forest_train_f1 = f1_score(forest_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for DTR Test Set:\n",
    "\n",
    "    forest_test_precision = precision_score(forest_pred_test, y_test)\n",
    "    forest_test_recall = recall_score(forest_pred_test, y_test)\n",
    "    forest_test_accuracy = accuracy_score(forest_pred_test, y_test)\n",
    "    forest_test_f1 = f1_score(forest_pred_test, y_test)\n",
    "    \n",
    "    ###Running Support Vector Machines###\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svc = SVC(kernel = 'linear')\n",
    "    \n",
    "    #Fit DTR model\n",
    "    \n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    svc_pred_train = svc.predict(X_train)\n",
    "    \n",
    "    svc_pred_test = svc.predict(X_test)\n",
    "\n",
    "    #Get Evaluation Metrics for DTR Train Set:\n",
    "    \n",
    "    svc_train_precision = precision_score(svc_pred_train, y_train)\n",
    "    svc_train_recall = recall_score(svc_pred_train, y_train)\n",
    "    svc_train_accuracy = accuracy_score(svc_pred_train, y_train)\n",
    "    svc_train_f1 = f1_score(svc_pred_train, y_train)\n",
    "    \n",
    "    #Get Evaluation Metrics for SVM Test Set:\n",
    "\n",
    "    svc_test_precision = precision_score(svc_pred_test, y_test)\n",
    "    svc_test_recall = recall_score(svc_pred_test, y_test)\n",
    "    svc_test_accuracy = accuracy_score(svc_pred_test, y_test)\n",
    "    svc_test_f1 = f1_score(svc_pred_test, y_test)\n",
    "    \n",
    "    ###Displaying Model Metrics###\n",
    "    \n",
    "    print(f'Logistic Regression Model Metrics:')\n",
    "    print(f'')\n",
    "    print(f'Logistic Regression Test Precision: {lr_test_precision}')\n",
    "    print(f'Logistic Regression Test Recall: {lr_test_recall}')\n",
    "    print(f'Logistic Regression Test Accuracy: {lr_test_accuracy}')\n",
    "    print(f'Logistic Regression Test F1: {lr_test_f1}')\n",
    "    print(f'---')\n",
    "    print(f'---')\n",
    "    \n",
    "    print(f'KNN Model Metrics:')\n",
    "    print(f'')\n",
    "    print(f'KNN Test Precision: {KNN_test_precision}')\n",
    "    print(f'KNN Test Recall: {KNN_test_recall}')\n",
    "    print(f'KNN Test Accuracy: {KNN_test_accuracy}')\n",
    "    print(f'KNN Test F1: {KNN_test_f1}')\n",
    "    print(f'---')\n",
    "    print(f'---')\n",
    "\n",
    "    print(f'Decision Tree Model Metrics:')\n",
    "    print(f'')\n",
    "    print(f'Decision Tree Test Precision: {DTC_test_precision}')\n",
    "    print(f'Decision Tree Test Recall: {DTC_test_recall}')\n",
    "    print(f'Decision Tree Test Accuracy: {DTC_test_accuracy}')\n",
    "    print(f'Decision Tree Test F1: {DTC_test_f1}')\n",
    "    print(f'---')\n",
    "    print(f'---')\n",
    "    \n",
    "    print(f'Decision Tree Regression Metrics:')\n",
    "    print(f'')\n",
    "    print(f'Decision Tree Regression Test Precision: {DTR_test_precision}')\n",
    "    print(f'Decision Tree Regression Test Recall: {DTR_test_recall}')\n",
    "    print(f'Decision Tree Regression Test Accuracy: {DTR_test_accuracy}')\n",
    "    print(f'Decision Tree Regression Test F1: {DTR_test_f1}')\n",
    "    print(f'---')\n",
    "    print(f'---')\n",
    "\n",
    "    print(f'Random Forest Metrics:')\n",
    "    print(f'')\n",
    "    print(f'Random Forest Test Precision: {forest_test_precision}')\n",
    "    print(f'Random Forest Test Recall: {forest_test_recall}')\n",
    "    print(f'Random Forest Test Accuracy: {forest_test_accuracy}')\n",
    "    print(f'Random Forest Test F1: {forest_test_f1}')\n",
    "    print(f'---')\n",
    "    print(f'---')\n",
    "\n",
    "    print(f'Support Vector Machine Metrics:')\n",
    "    print(f'')\n",
    "    print(f'Support Vector Machine Test Precision: {svc_test_precision}')\n",
    "    print(f'Support Vector Machine Test Recall: {svc_test_recall}')\n",
    "    print(f'Support Vector Machine Test Accuracy: {svc_test_accuracy}')\n",
    "    print(f'Support Vector Machine Test F1: {svc_test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Metrics:\n",
      "\n",
      "Logistic Regression Test Precision: 0.16666666666666666\n",
      "Logistic Regression Test Recall: 0.5909090909090909\n",
      "Logistic Regression Test Accuracy: 0.852\n",
      "Logistic Regression Test F1: 0.26\n",
      "---\n",
      "---\n",
      "KNN Model Metrics:\n",
      "\n",
      "KNN Test Precision: 0.34615384615384615\n",
      "KNN Test Recall: 0.84375\n",
      "KNN Test Accuracy: 0.888\n",
      "KNN Test F1: 0.4909090909090909\n",
      "---\n",
      "---\n",
      "Decision Tree Model Metrics:\n",
      "\n",
      "Decision Tree Test Precision: 0.7435897435897436\n",
      "Decision Tree Test Recall: 0.6444444444444445\n",
      "Decision Tree Test Accuracy: 0.896\n",
      "Decision Tree Test F1: 0.6904761904761906\n",
      "---\n",
      "---\n",
      "Decision Tree Regression Metrics:\n",
      "\n",
      "Decision Tree Regression Test Precision: 0.7692307692307693\n",
      "Decision Tree Regression Test Recall: 0.7058823529411765\n",
      "Decision Tree Regression Test Accuracy: 0.914\n",
      "Decision Tree Regression Test F1: 0.736196319018405\n",
      "---\n",
      "---\n",
      "Random Forest Metrics:\n",
      "\n",
      "Random Forest Test Precision: 0.4358974358974359\n",
      "Random Forest Test Recall: 1.0\n",
      "Random Forest Test Accuracy: 0.912\n",
      "Random Forest Test F1: 0.6071428571428571\n",
      "---\n",
      "---\n",
      "Support Vector Machine Metrics:\n",
      "\n",
      "Support Vector Machine Test Precision: 0.0\n",
      "Support Vector Machine Test Recall: 0.0\n",
      "Support Vector Machine Test Accuracy: 0.844\n",
      "Support Vector Machine Test F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelogayanelo/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Running a Multi Model Assessment to determine which model best understands the data set\n",
    "\n",
    "multi_model_test(X_train_proc, y_train, X_test_proc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Multi Model Findings\n",
    "\n",
    "1. The highest F1 scores come from Decision Tree Classifiers and Decission Tree Regression\n",
    "2. Random Forest Metrics are showing very promising Accuracy and Recall, however Precision is very low\n",
    "3. Scores may be improved accross all models by first solving for the Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Metrics:\n",
      "\n",
      "Decision Tree Test Precision: 0.8076923076923077\n",
      "Decision Tree Test Recall: 0.7590361445783133\n",
      "Decision Tree Test Accuracy: 0.93\n",
      "Decision Tree Test F1: 0.7826086956521741\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#Checking Balanced Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "   \n",
    "model = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "\n",
    "model.fit(X_train_proc, y_train)\n",
    "\n",
    "model_pred_train = model.predict(X_train_proc)\n",
    "\n",
    "model_pred_test = model.predict(X_test_proc)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Train Set:\n",
    "\n",
    "model_train_precision = precision_score(model_pred_train, y_train)\n",
    "model_train_recall = recall_score(model_pred_train, y_train)\n",
    "model_train_accuracy = accuracy_score(model_pred_train, y_train)\n",
    "model_train_f1 = f1_score(model_pred_train, y_train)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Test Set:\n",
    "\n",
    "model_test_precision = precision_score(model_pred_test, y_test)\n",
    "model_test_recall = recall_score(model_pred_test, y_test)\n",
    "model_test_accuracy = accuracy_score(model_pred_test, y_test)\n",
    "model_test_f1 = f1_score(model_pred_test, y_test)\n",
    "\n",
    "print(f'Decision Tree Model Metrics:')\n",
    "print(f'')\n",
    "print(f'Decision Tree Test Precision: {model_test_precision}')\n",
    "print(f'Decision Tree Test Recall: {model_test_recall}')\n",
    "print(f'Decision Tree Test Accuracy: {model_test_accuracy}')\n",
    "print(f'Decision Tree Test F1: {model_test_f1}')\n",
    "print(f'---')\n",
    "print(f'---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Metrics:\n",
      "\n",
      "Model Test Precision: 0.7564102564102564\n",
      "Model Test Recall: 0.38064516129032255\n",
      "Model Test Accuracy: 0.77\n",
      "Model Test F1: 0.5064377682403433\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#Checking Balanced Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "   \n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "\n",
    "model.fit(X_train_proc, y_train)\n",
    "\n",
    "model_pred_train = model.predict(X_train_proc)\n",
    "\n",
    "model_pred_test = model.predict(X_test_proc)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Train Set:\n",
    "\n",
    "model_train_precision = precision_score(model_pred_train, y_train)\n",
    "model_train_recall = recall_score(model_pred_train, y_train)\n",
    "model_train_accuracy = accuracy_score(model_pred_train, y_train)\n",
    "model_train_f1 = f1_score(model_pred_train, y_train)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Test Set:\n",
    "\n",
    "model_test_precision = precision_score(model_pred_test, y_test)\n",
    "model_test_recall = recall_score(model_pred_test, y_test)\n",
    "model_test_accuracy = accuracy_score(model_pred_test, y_test)\n",
    "model_test_f1 = f1_score(model_pred_test, y_test)\n",
    "\n",
    "print(f'Logistic Regression Model Metrics:')\n",
    "print(f'')\n",
    "print(f'Model Test Precision: {model_test_precision}')\n",
    "print(f'Model Test Recall: {model_test_recall}')\n",
    "print(f'Model Test Accuracy: {model_test_accuracy}')\n",
    "print(f'Model Test F1: {model_test_f1}')\n",
    "print(f'---')\n",
    "print(f'---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Metrics:\n",
      "\n",
      "Model Test Precision: 0.717948717948718\n",
      "Model Test Recall: 0.9824561403508771\n",
      "Model Test Accuracy: 0.954\n",
      "Model Test F1: 0.8296296296296296\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#Checking Balanced Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "   \n",
    "model = RandomForestClassifier(class_weight = 'balanced')\n",
    "\n",
    "model.fit(X_train_proc, y_train)\n",
    "\n",
    "model_pred_train = model.predict(X_train_proc)\n",
    "\n",
    "model_pred_test = model.predict(X_test_proc)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Train Set:\n",
    "\n",
    "model_train_precision = precision_score(model_pred_train, y_train)\n",
    "model_train_recall = recall_score(model_pred_train, y_train)\n",
    "model_train_accuracy = accuracy_score(model_pred_train, y_train)\n",
    "model_train_f1 = f1_score(model_pred_train, y_train)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Test Set:\n",
    "\n",
    "model_test_precision = precision_score(model_pred_test, y_test)\n",
    "model_test_recall = recall_score(model_pred_test, y_test)\n",
    "model_test_accuracy = accuracy_score(model_pred_test, y_test)\n",
    "model_test_f1 = f1_score(model_pred_test, y_test)\n",
    "\n",
    "print(f'Random Forest Model Metrics:')\n",
    "print(f'')\n",
    "print(f'Model Test Precision: {model_test_precision}')\n",
    "print(f'Model Test Recall: {model_test_recall}')\n",
    "print(f'Model Test Accuracy: {model_test_accuracy}')\n",
    "print(f'Model Test F1: {model_test_f1}')\n",
    "print(f'---')\n",
    "print(f'---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weight Analysis\n",
    "\n",
    "1. Random Forests yielded the greatest improvement in total F1\n",
    "2. Deision Tree Classifiers came in a close second\n",
    "3. Logistic Regression maintained a relatively low score\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Run CV/Grid Search on Random Forest Classifier\n",
    "2. Determine feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(class_weight='balanced'),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [None, 2, 3, 4, 5, 6],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6],\n",
       "                         'min_samples_split': [2, 5, 10]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Grid Search Parameters\n",
    "\n",
    "# # dt_param_grid = {\n",
    "# #     'criterion': ['gini', 'entropy'],\n",
    "# #     'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "# #     'min_samples_split': [2, 5, 10],\n",
    "# #     'min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "# # }\n",
    "\n",
    "# dt_param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "#     'min_samples_split': [ 2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "# }\n",
    "\n",
    "# #Instantiate Grid Search CV\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid = GridSearchCV(model, dt_param_grid, cv = 3, scoring = 'f1')\n",
    "\n",
    "# grid.fit(X_train_proc, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Metrics: criterion: entropy, max_depth: None, min_samples_leaf: 5, min_samples_split: 2\n"
     ]
    }
   ],
   "source": [
    "# #Evaluating Metrics from Gridsearch CV:\n",
    "\n",
    "# print(f'Best Score: {grid.best_score_}')\n",
    "\n",
    "# #Evaluating best\n",
    "\n",
    "# print(f'Best Parameters: {grid.best_params_}')\n",
    "\n",
    "print(f'Best Metrics: criterion: entropy, max_depth: None, min_samples_leaf: 5, min_samples_split: 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Metrics:\n",
      "\n",
      "Model Test Precision: 0.8333333333333334\n",
      "Model Test Recall: 0.8783783783783784\n",
      "Model Test Accuracy: 0.956\n",
      "Model Test F1: 0.855263157894737\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(class_weight = 'balanced', \n",
    "                               criterion = 'entropy', \n",
    "                               max_depth = None, \n",
    "                               min_samples_leaf = 5, \n",
    "                               min_samples_split = 2)\n",
    "\n",
    "model.fit(X_train_proc, y_train)\n",
    "\n",
    "model_pred_train = model.predict(X_train_proc)\n",
    "\n",
    "model_pred_test = model.predict(X_test_proc)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Train Set:\n",
    "\n",
    "model_train_precision = precision_score(model_pred_train, y_train)\n",
    "model_train_recall = recall_score(model_pred_train, y_train)\n",
    "model_train_accuracy = accuracy_score(model_pred_train, y_train)\n",
    "model_train_f1 = f1_score(model_pred_train, y_train)\n",
    "\n",
    "#Get Evaluation Metrics for DTC Test Set:\n",
    "\n",
    "model_test_precision = precision_score(model_pred_test, y_test)\n",
    "model_test_recall = recall_score(model_pred_test, y_test)\n",
    "model_test_accuracy = accuracy_score(model_pred_test, y_test)\n",
    "model_test_f1 = f1_score(model_pred_test, y_test)\n",
    "\n",
    "print(f'Random Forest Model Metrics:')\n",
    "print(f'')\n",
    "print(f'Model Test Precision: {model_test_precision}')\n",
    "print(f'Model Test Recall: {model_test_recall}')\n",
    "print(f'Model Test Accuracy: {model_test_accuracy}')\n",
    "print(f'Model Test F1: {model_test_f1}')\n",
    "print(f'---')\n",
    "print(f'---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV Results\n",
    "\n",
    "Overall scoring has gone up significantly, with Accuracy hitting 95% and F1 increasing to 85%\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Determine Feature Importance\n",
    "2. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
